{"cells":[{"cell_type":"markdown","source":["# 참고 Data Types (in RDD-based API Guide) \n\n* MLib suports \n  - local vectors and matrices stored on a single machine. \n  - distributed matrices backed by one or more RDDs.\n \n* Local vectors and local matrices are simple data models that serve as public interfaces.\n*  The underlying linear algebra operations are provided by Breeze. \n\n## Local vector\n* Values: integer-typed and 0-based indices and double-typed values, stored on a single machine. \n* two types of local vectors: dense and sparse.\n* (1.0, 0.0, 3.0) \n   - --> dense format: [1.0, 0.0, 3.0] \n   - --> sparse format: (3, [0, 2], [1.0, 3.0])\n     - 3 : size of vector \n     - [0,2]: non 0 postion\n     - [1.0, 3.0]: non 0 values\n     \n* MLlib recognizes the following types \n  + as dense vectors:\n    - NumPy’s array\n    - Python’s list, e.g., [1, 2, 3]\n  + as sparse vectors\n    - MLlib’s SparseVector.\n    - SciPy’s csc_matrix with a single column\n    \n* We recommend using NumPy arrays over lists for efficiency, and using the factory methods implemented in Vectors to create sparse vectors."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f534bcd8-6b4f-4d2f-b4b5-987fd5a41526"}}},{"cell_type":"code","source":["import numpy as np\nimport scipy.sparse as sps\nfrom pyspark.mllib.linalg import Vectors\n\n# Use a NumPy array as a dense vector.\ndv1 = np.array([1.0, 0.0, 3.0])\n# Use a Python list as a dense vector.\ndv2 = [1.0, 0.0, 3.0]\n# Create a SparseVector.\nsv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9c49936-66dd-4207-a4d8-654c0f6c9585"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Labeled point\n\n* local vector, either dense or sparse, associated with a label/response.\n* In MLlib, labeled points are used in supervised learning algorithms. \n\n* A labeled point is represented by LabeledPoint.\n* (참고) https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.mllib.regression.LabeledPoint.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"559e2238-05cc-47a2-aca3-c8be2796447e"}}},{"cell_type":"code","source":["from pyspark.mllib.linalg import SparseVector\nfrom pyspark.mllib.regression import LabeledPoint\n\n# Create a labeled point with a positive label and a dense feature vector.\npos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n\n# Create a labeled point with a negative label and a sparse feature vector.\nneg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b7de33f-9d8e-4b2b-b0e7-1c115438e3a5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Sparse data\n* It is very common in practice to have sparse training data.\n* MLlib supports reading training examples stored in LIBSVM format, which is the default format used by LIBSVM and LIBLINEAR.\n> label index1:value1 index2:value2 ...\n  - where the indices are one-based and in ascending order. After loading, the feature indices are converted to zero-based.\n  \n* MLUtils.loadLibSVMFile reads training examples stored in LIBSVM format.\n  - https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.mllib.util.MLUtils.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa5b67ed-4d50-4143-9d6c-5214a8c83fb2"}}},{"cell_type":"code","source":["from pyspark.mllib.util import MLUtils\nexamples = MLUtils.loadLibSVMFile(sc, \"/FileStore/sample_lda_libsvm_data.txt\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c64aa3c6-03e0-4055-9a6c-16a34c25bb62"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### A local matrix \n* A local matrix has integer-typed row and column indices and double-typed values, stored on a single machine. \n* MLlib supports \n  + dense matrices, whose entry values are stored in a single double array in column-major order, \n  + and sparse matrices, whose non-zero entry values are stored in the Compressed Sparse Column (CSC) format in column-major order. \n* For example, the following dense matrix \n  - (3x3 matrix)\n  \n    (1.0 2.0)\n    \n    (3.0 4.0)\n    \n    (5.0 6.0)\n    \n* The base class of local matrices is Matrix, and we provide two implementations: DenseMatrix, and SparseMatrix. We recommend using the factory methods implemented in Matrices to create local matrices. Remember, local matrices in MLlib are stored in column-major order.\n\n* Refer to the Matrix Python docs and Matrices Python docs for more details on the API.\n  - https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.mllib.linalg.Matrix.html\n  - https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.mllib.linalg.Matrices.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ba2b700-d263-40bf-afa6-cd4eed357cf9"}}},{"cell_type":"code","source":["from pyspark.mllib.linalg import Matrix, Matrices\n\n# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\ndm2 = Matrices.dense(3, 2, [1, 3, 5, 2, 4, 6])\n\n# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\nsm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cce724b9-917f-4ed6-a85c-bf7743aa7aba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Distributed matrix\n* A distributed matrix has long-typed row and column indices and double-typed values, stored distributively in one or more RDDs. \n* Four types of distributed matrices have been implemented so far.\n    + RowMatrix\n        - a row-oriented distributed matrix without meaningful row indices, e.g., a collection of feature vectors. \n        - It is backed by an RDD of its rows, where each row is a local vector.\n        - RowMatrix can be created from an RDD of vectors.\n        - https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.mllib.linalg.distributed.RowMatrix.html\n    + IndexedRowMatrix \n        - similar to a RowMatrix but with row indices, which can be used for identifying rows and executing joins. \n        - It is backed by an RDD of indexed rows, so that each row is represented by its index (long-typed) and a local vector.\n        - An IndexedRowMatrix can be converted to a RowMatrix by dropping its row indices.\n        - https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.mllib.linalg.distributed.IndexedRowMatrix.html\n    + CoordinateMatrix     \n        - a distributed matrix stored in coordinate list (COO) format, backed by an RDD of its entries.\n        - Each entry is a tuple of (i: Long, j: Long, value: Double), where i is the row index, j is the column index, and value is the entry value. \n        - A CoordinateMatrix should be used only when both dimensions of the matrix are huge and the matrix is very sparse.\n        - https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.mllib.linalg.distributed.CoordinateMatrix.html\n    + BlockMatrix \n        -  a distributed matrix backed by an RDD of MatrixBlock which is a tuple of (Int, Int, Matrix),  where the (Int, Int) is the index of the block, and Matrix is the sub-matrix at the given index with size rowsPerBlock x colsPerBlock.\n        - BlockMatrix supports methods such as add and multiply with another BlockMatrix. \n        - BlockMatrix also has a helper function validate which can be used to check whether the BlockMatrix is set up properly.\n        - https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.mllib.linalg.distributed.BlockMatrix.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ece32224-d10a-4a57-80e0-7e94a18d5dd3"}}},{"cell_type":"code","source":["# RowMatrix\n\nfrom pyspark.mllib.linalg.distributed import RowMatrix\n\n# Create an RDD of vectors.\nrows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\n# Create a RowMatrix from an RDD of vectors.\nmat = RowMatrix(rows)\n\n# Get its size.\nm = mat.numRows()  # 4\nn = mat.numCols()  # 3\n\n# Get the rows as an RDD of vectors again.\nrowsRDD = mat.rows\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e55bd832-9caf-466e-8f0e-ad0f7a7b09a0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# IndexedRowMatrix\n\nfrom pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n\n# Create an RDD of indexed rows.\n#   - This can be done explicitly with the IndexedRow class:\nindexedRows = sc.parallelize([IndexedRow(0, [1, 2, 3]),\n                              IndexedRow(1, [4, 5, 6]),\n                              IndexedRow(2, [7, 8, 9]),\n                              IndexedRow(3, [10, 11, 12])])\n#   - or by using (long, vector) tuples:\nindexedRows = sc.parallelize([(0, [1, 2, 3]), (1, [4, 5, 6]),\n                              (2, [7, 8, 9]), (3, [10, 11, 12])])\n\n# Create an IndexedRowMatrix from an RDD of IndexedRows.\nmat = IndexedRowMatrix(indexedRows)\n\n# Get its size.\nm = mat.numRows()  # 4\nn = mat.numCols()  # 3\n\n# Get the rows as an RDD of IndexedRows.\nrowsRDD = mat.rows\n\n# Convert to a RowMatrix by dropping the row indices.\nrowMat = mat.toRowMatrix()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b657a0e3-9d03-47af-918e-72249af40101"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# CoordinateMatrix\n\nfrom pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n\n# Create an RDD of coordinate entries.\n#   - This can be done explicitly with the MatrixEntry class:\nentries = sc.parallelize([MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(2, 1, 3.7)])\n#   - or using (long, long, float) tuples:\nentries = sc.parallelize([(0, 0, 1.2), (1, 0, 2.1), (2, 1, 3.7)])\n\n# Create a CoordinateMatrix from an RDD of MatrixEntries.\nmat = CoordinateMatrix(entries)\n\n# Get its size.\nm = mat.numRows()  # 3\nn = mat.numCols()  # 2\n\n# Get the entries as an RDD of MatrixEntries.\nentriesRDD = mat.entries\n\n# Convert to a RowMatrix.\nrowMat = mat.toRowMatrix()\n\n# Convert to an IndexedRowMatrix.\nindexedRowMat = mat.toIndexedRowMatrix()\n\n# Convert to a BlockMatrix.\nblockMat = mat.toBlockMatrix()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"270dde14-68c3-47b9-9203-b9f98b361ee1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# BlockMatrix\n\nfrom pyspark.mllib.linalg import Matrices\nfrom pyspark.mllib.linalg.distributed import BlockMatrix\n\n# Create an RDD of sub-matrix blocks.\nblocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\n                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n\n# Create a BlockMatrix from an RDD of sub-matrix blocks.\nmat = BlockMatrix(blocks, 3, 2)\n\n# Get its size.\nm = mat.numRows()  # 6\nn = mat.numCols()  # 2\n\n# Get the blocks as an RDD of sub-matrix blocks.\nblocksRDD = mat.blocks\n\n# Convert to a LocalMatrix.\nlocalMat = mat.toLocalMatrix()\n\n# Convert to an IndexedRowMatrix.\nindexedRowMat = mat.toIndexedRowMatrix()\n\n# Convert to a CoordinateMatrix.\ncoordinateMat = mat.toCoordinateMatrix()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc5d3234-b7bc-44fd-8ea8-ed3faf12781a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/spark/python/pyspark/sql/context.py:134: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/spark/python/pyspark/sql/context.py:134: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"6.Appendix Data Types","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2850831041173890}},"nbformat":4,"nbformat_minor":0}
