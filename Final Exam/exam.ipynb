{"cells":[{"cell_type":"code","source":["# 데이터는 100배수만을 이용. (0, 100, 200, ...번째 데이터) \nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\ndata, target = fetch_california_housing(as_frame=True, return_X_y=True)\n\nfor index, row in data.iterrows():\n    if(index % 100 != 0):\n        data = data.drop(index=[index])\n# print(data)\n\ntarget = target.to_frame()\nfor index, row in target.iterrows():\n    if(index % 100 != 0):\n        target = target.drop(index=[index])\n# print(target)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a24159d3-d6c4-45b1-94d7-17be5480e575"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# 1. SparkSQL을 이용하여 수행하라. target의 평균값을 출력하라. \n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import avg\n\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark SQL basic example\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\ndf = spark.createDataFrame(target)\n# df.show()\ndf.select(avg(\"MedHouseVal\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff862818-ac36-4200-9a85-c88f5d91ebec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------------------+\n|  avg(MedHouseVal)|\n+------------------+\n|2.0160438164251206|\n+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------------------+\n|  avg(MedHouseVal)|\n+------------------+\n|2.0160438164251206|\n+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# 2.(2) 입력변수 중 임의개수를 활용하여 5개로 클러스터링하라. (클러스터링 알고리즘은 자유이나 Spark 이용) 그리고, 각 클러스터의 centroid를 출력하라. \nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\n\ndataset = spark.createDataFrame(data)\n# dataset.show()\n\n# 5개\nhousing_df = dataset.select('MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population')\nfeatureCols = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population']\nassembler = VectorAssembler(inputCols=featureCols, outputCol='features')\nassembled_df = assembler.transform(dataset)\n# assembled_df.show(10, truncate=True)\n\nkmeans = KMeans().setK(5).setSeed(1)\nmodel = kmeans.fit(assembled_df)\n\n# Make predictions\npredictions = model.transform(assembled_df)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"253fcbde-044a-4428-ad24-0879c09c02fc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Cluster Centers: \n[3.79682609e+00 3.08550725e+01 5.12816717e+00 1.05730848e+00\n 1.24260870e+03]\n[6.06660000e+00 1.00000000e+01 6.52651839e+00 9.73053892e-01\n 7.26600000e+03]\n[4.48612500e+00 2.23333333e+01 5.31802195e+00 1.05475252e+00\n 3.55308333e+03]\n[3.27541364e+00 2.38863636e+01 5.08001723e+00 1.09532096e+00\n 2.02552273e+03]\n[  3.82294568  30.16049383   5.56381394   1.10849192 637.95061728]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Cluster Centers: \n[3.79682609e+00 3.08550725e+01 5.12816717e+00 1.05730848e+00\n 1.24260870e+03]\n[6.06660000e+00 1.00000000e+01 6.52651839e+00 9.73053892e-01\n 7.26600000e+03]\n[4.48612500e+00 2.23333333e+01 5.31802195e+00 1.05475252e+00\n 3.55308333e+03]\n[3.27541364e+00 2.38863636e+01 5.08001723e+00 1.09532096e+00\n 2.02552273e+03]\n[  3.82294568  30.16049383   5.56381394   1.10849192 637.95061728]\n"]}}],"execution_count":0},{"cell_type":"code","source":["# 3. (3점) 2번에서 선택해서 푼 결과를 이용하여, 전체 데이터에 대하여 실행하여 결과를 출력하는 프로그램을 작성하시오. \n\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\n\ndata_all, target_all = fetch_california_housing(as_frame=True, return_X_y=True)\n\ndataset = spark.createDataFrame(data_all)\n# dataset.show()\n\n# 5개\nhousing_df = dataset.select('MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population')\nfeatureCols = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population']\nassembler = VectorAssembler(inputCols=featureCols, outputCol='features')\nassembled_df = assembler.transform(dataset)\n# assembled_df.show(10, truncate=True)\n\nkmeans = KMeans().setK(5).setSeed(1)\nmodel = kmeans.fit(assembled_df)\n\n# Make predictions\npredictions = model.transform(assembled_df)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f34309ee-2d92-4c96-a26d-c2b950b4b43c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Cluster Centers: \n[  3.93843525  31.78187609   5.68724719   1.13220554 724.23158327]\n[3.96513270e+00 1.74881517e+01 5.25773486e+00 1.07317762e+00\n 4.63796051e+03]\n[3.77512292e+00 2.23812000e+01 5.07518521e+00 1.06692280e+00\n 2.67109360e+03]\n[3.79580120e+00 2.78984559e+01 5.22424049e+00 1.06346436e+00\n 1.51498260e+03]\n[4.56522803e+00 1.18409091e+01 5.76380922e+00 1.07572319e+00\n 8.95328788e+03]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Cluster Centers: \n[  3.93843525  31.78187609   5.68724719   1.13220554 724.23158327]\n[3.96513270e+00 1.74881517e+01 5.25773486e+00 1.07317762e+00\n 4.63796051e+03]\n[3.77512292e+00 2.23812000e+01 5.07518521e+00 1.06692280e+00\n 2.67109360e+03]\n[3.79580120e+00 2.78984559e+01 5.22424049e+00 1.06346436e+00\n 1.51498260e+03]\n[4.56522803e+00 1.18409091e+01 5.76380922e+00 1.07572319e+00\n 8.95328788e+03]\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2019156037최혜민","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2213412329745040}},"nbformat":4,"nbformat_minor":0}
